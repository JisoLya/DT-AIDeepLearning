理解你的需求，我已经将内容重新按照原模板格式进行排版，如下所示：

---

## 一、网络结构

![LeNet-5 结构](https://github.com/buluslee/DT-AI/assets/93359778/484457af-d8c8-4fba-a46e-3ee160ac5b2f)


**LeNet-5** 是一个经典的卷积神经网络，它的网络结构如上图所示。整个网络包含了输入层、卷积层、池化层、激活函数、全连接层和输出层。具体结构为：输入层 -> 卷积层 -> 池化层 -> 激活函数 -> 卷积层 -> 池化层 -> 激活函数 -> 卷积层 -> 全连接层 -> 全连接层 -> 输出层。

LeNet-5 网络一共包括了7层（不包括输入层），命名为 C1、S2、C3、S4、C5、F6、OUTPUT。

输入层（INPUT）是 32x32 像素的图像，注意通道数为1。

### 几个重要参数：

- 层编号特点：英文字母 + 数字，其中英文字母代表不同类型的层（C：卷积层，S：池化层，F：全连接层），数字代表第几层。
- 每个卷积核对应一个权重 w 和一个偏置 b。

### 层次解释：

- **C1 层**：第一个卷积层，使用6个大小为5x5的卷积核，padding=0，stride=1 进行卷积，得到6个大小为28x28的特征图。 ![C1 层](https://github.com/buluslee/DT-AI/assets/93359778/07dabd5d-7dc5-43e8-a34e-1d73060a369d)

  ***参数个数***：(5 * 5 + 1) * 6 = 156，其中5 * 5为卷积核的25个参数w，1为偏置项b。
  
  ***连接数***：156 * 28 * 28=122304，其中156为单次卷积过程连线数，28 * 28为输出特征层，每一个像素都由前面卷积得到，即总共经历28 * 28次卷积。 

- **S2 层**：第一个池化层，使用6个大小为2x2的卷积核，padding=0，stride=2 进行池化操作，得到6个大小为14x14的特征图。 ![S2 层](https://github.com/buluslee/DT-AI/assets/93359778/372d615a-a1c4-4675-b08d-92961cb7e72f)

     S2 层其实相当于降采样层+激活层。先是降采样，然后激活函数 sigmoid 非线性输出。先对 C1 层 2x2 的视野求和，然后进入激活函数，即： ![sigmode激活函数](https://github.com/buluslee/DT-AI/assets/93359778/16a27f09-bf7f-42fb-b5f5-2361161747bb)


- **C3 层**：第二个卷积层，使用16个大小为5x5xn（不同组的输入）的卷积核，padding=0，stride=1 进行卷积操作，得到16个大小为10x10的特征图。

- **S4 层**：第二个池化层，使用16个大小为2x2的卷积核，padding=0，stride=2 进行池化操作，得到16个大小为5x5的特征图。

- **C5 层**：第三个卷积层，使用120个大小为5x5x16的卷积核，padding=0，stride=1 进行卷积操作，得到120个大小为1x1的特征图。

- **F6 层**：全连接层，共有84个神经元，与 C5 层全连接。

- **Output 层**：输出层，采用 RBF 函数计算输入向量与参数向量之间的欧式距离。

### 参数与连接数量：

- C1 层：参数156，连接数122,304。
- S2 层：参数12，连接数5,880。
- C3 层：参数1,516，连接数151,600。
- S4 层：参数32，连接数2,000。
- C5 层：参数48,120，连接数48,120。
- F6 层：参数10,164，连接数10,164。
- Output 层：参数840，连接数840。

## 二、总结

LeNet-5共7层（不含输入层），通过卷积、池化、全连接等层次操作，实现对手写体字符的高效识别。尽管与现代卷积神经网络在细节上存在差异，如激活函数、池化处理和输出层的选择，但LeNet-5为CNN的发展奠定了基础，通过有效地提取图像特征，使得从原始像素中识别规律变得更加容易。

虽然 LeNet-5 在一些细节上与现代卷积神经网络存在差异，如激活函数、池化层处理和输出层的选择，但它为 CNN 的发展奠定了基础。通过有效地提取图像特征，LeNet-5 在早期就实现了对手写字符的高效识别。虽然 LeNet-5 在当时面临数据量不足和计算能力有限的挑战，但它为深度学习的发展铺平了道路。
